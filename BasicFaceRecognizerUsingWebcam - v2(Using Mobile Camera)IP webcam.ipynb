{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic face recognizer using pre trained model\n",
    "\n",
    "The implementation is inspired by two path breaking papers on facial recognition using deep convoluted neural network, namely FaceNet and DeepFace.\n",
    "\n",
    "It follows 2 parts:- \n",
    "\n",
    "*Face Verification* - \"is this the claimed person?\". For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.\n",
    "\n",
    "*Face Recognition* - \"who is this person?\". For example, the video lecture showed a face recognition video (https://www.youtube.com/watch?v=wr4rx0Spihs) of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the prerequisite libraries\n",
    "\n",
    "We will be importing utils.py from https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/utils.py (available with code) which contains utility functions to create the neural network and load the weights assoiated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import LRN2D\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contructing the neural network model\n",
    "The model here constructed is based on FaceNet's Inception model.\n",
    "\n",
    "The implementation of model is available at: https://github.com/iwantooxxoox/Keras-OpenFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myInput = Input(shape=(96, 96, 3))\n",
    "\n",
    "x = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n",
    "x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
    "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "x = Lambda(LRN2D, name='lrn_1')(x)\n",
    "x = Conv2D(64, (1, 1), name='conv2')(x)\n",
    "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = Conv2D(192, (3, 3), name='conv3')(x)\n",
    "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Lambda(LRN2D, name='lrn_2')(x)\n",
    "x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "# Inception3a\n",
    "inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n",
    "inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n",
    "inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
    "inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n",
    "inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n",
    "inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n",
    "inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
    "\n",
    "inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n",
    "inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n",
    "inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
    "inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n",
    "inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n",
    "inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n",
    "inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
    "\n",
    "inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n",
    "inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n",
    "inception_3a_pool = Activation('relu')(inception_3a_pool)\n",
    "inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n",
    "\n",
    "inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n",
    "inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n",
    "inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n",
    "\n",
    "inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n",
    "\n",
    "# Inception3b\n",
    "inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n",
    "inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n",
    "inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
    "inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n",
    "inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n",
    "inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n",
    "inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
    "\n",
    "inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n",
    "inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n",
    "inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
    "inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n",
    "inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n",
    "inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n",
    "inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
    "\n",
    "inception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\n",
    "inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\n",
    "inception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\n",
    "inception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\n",
    "inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n",
    "inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n",
    "inception_3b_pool = Activation('relu')(inception_3b_pool)\n",
    "inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n",
    "\n",
    "inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n",
    "inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n",
    "inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n",
    "\n",
    "inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n",
    "\n",
    "# Inception3c\n",
    "inception_3c_3x3 = utils.conv2d_bn(inception_3b,\n",
    "                                   layer='inception_3c_3x3',\n",
    "                                   cv1_out=128,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=256,\n",
    "                                   cv2_filter=(3, 3),\n",
    "                                   cv2_strides=(2, 2),\n",
    "                                   padding=(1, 1))\n",
    "\n",
    "inception_3c_5x5 = utils.conv2d_bn(inception_3b,\n",
    "                                   layer='inception_3c_5x5',\n",
    "                                   cv1_out=32,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=64,\n",
    "                                   cv2_filter=(5, 5),\n",
    "                                   cv2_strides=(2, 2),\n",
    "                                   padding=(2, 2))\n",
    "\n",
    "inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n",
    "inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n",
    "\n",
    "inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n",
    "\n",
    "#inception 4a\n",
    "inception_4a_3x3 = utils.conv2d_bn(inception_3c,\n",
    "                                   layer='inception_4a_3x3',\n",
    "                                   cv1_out=96,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=192,\n",
    "                                   cv2_filter=(3, 3),\n",
    "                                   cv2_strides=(1, 1),\n",
    "                                   padding=(1, 1))\n",
    "inception_4a_5x5 = utils.conv2d_bn(inception_3c,\n",
    "                                   layer='inception_4a_5x5',\n",
    "                                   cv1_out=32,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=64,\n",
    "                                   cv2_filter=(5, 5),\n",
    "                                   cv2_strides=(1, 1),\n",
    "                                   padding=(2, 2))\n",
    "\n",
    "inception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\n",
    "inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\n",
    "inception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\n",
    "inception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\n",
    "inception_4a_pool = utils.conv2d_bn(inception_4a_pool,\n",
    "                                   layer='inception_4a_pool',\n",
    "                                   cv1_out=128,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   padding=(2, 2))\n",
    "inception_4a_1x1 = utils.conv2d_bn(inception_3c,\n",
    "                                   layer='inception_4a_1x1',\n",
    "                                   cv1_out=256,\n",
    "                                   cv1_filter=(1, 1))\n",
    "inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n",
    "\n",
    "#inception4e\n",
    "inception_4e_3x3 = utils.conv2d_bn(inception_4a,\n",
    "                                   layer='inception_4e_3x3',\n",
    "                                   cv1_out=160,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=256,\n",
    "                                   cv2_filter=(3, 3),\n",
    "                                   cv2_strides=(2, 2),\n",
    "                                   padding=(1, 1))\n",
    "inception_4e_5x5 = utils.conv2d_bn(inception_4a,\n",
    "                                   layer='inception_4e_5x5',\n",
    "                                   cv1_out=64,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=128,\n",
    "                                   cv2_filter=(5, 5),\n",
    "                                   cv2_strides=(2, 2),\n",
    "                                   padding=(2, 2))\n",
    "inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n",
    "inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n",
    "\n",
    "inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n",
    "\n",
    "#inception5a\n",
    "inception_5a_3x3 = utils.conv2d_bn(inception_4e,\n",
    "                                   layer='inception_5a_3x3',\n",
    "                                   cv1_out=96,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=384,\n",
    "                                   cv2_filter=(3, 3),\n",
    "                                   cv2_strides=(1, 1),\n",
    "                                   padding=(1, 1))\n",
    "\n",
    "inception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\n",
    "inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\n",
    "inception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\n",
    "inception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\n",
    "inception_5a_pool = utils.conv2d_bn(inception_5a_pool,\n",
    "                                   layer='inception_5a_pool',\n",
    "                                   cv1_out=96,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   padding=(1, 1))\n",
    "inception_5a_1x1 = utils.conv2d_bn(inception_4e,\n",
    "                                   layer='inception_5a_1x1',\n",
    "                                   cv1_out=256,\n",
    "                                   cv1_filter=(1, 1))\n",
    "\n",
    "inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n",
    "\n",
    "#inception_5b\n",
    "inception_5b_3x3 = utils.conv2d_bn(inception_5a,\n",
    "                                   layer='inception_5b_3x3',\n",
    "                                   cv1_out=96,\n",
    "                                   cv1_filter=(1, 1),\n",
    "                                   cv2_out=384,\n",
    "                                   cv2_filter=(3, 3),\n",
    "                                   cv2_strides=(1, 1),\n",
    "                                   padding=(1, 1))\n",
    "inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n",
    "inception_5b_pool = utils.conv2d_bn(inception_5b_pool,\n",
    "                                   layer='inception_5b_pool',\n",
    "                                   cv1_out=96,\n",
    "                                   cv1_filter=(1, 1))\n",
    "inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n",
    "\n",
    "inception_5b_1x1 = utils.conv2d_bn(inception_5a,\n",
    "                                   layer='inception_5b_1x1',\n",
    "                                   cv1_out=256,\n",
    "                                   cv1_filter=(1, 1))\n",
    "inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n",
    "\n",
    "av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n",
    "reshape_layer = Flatten()(av_pool)\n",
    "dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n",
    "norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
    "\n",
    "\n",
    "# Final Model\n",
    "model = Model(inputs=[myInput], outputs=norm_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model with pretrained weights\n",
    "\n",
    "FaceNet is trained by minimizing the triplet loss. I will be  loading a previously trained model. weights are avaiable at https://github.com/iwantooxxoox/Keras-OpenFace in the \"weights\" folder which is also provided in this source.\n",
    "\n",
    "This can take a couple of minutes to execute and depends on the speed of your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from csv files (which was exported from Openface torch model)\n",
    "weights = utils.weights\n",
    "weights_dict = utils.load_weights()\n",
    "\n",
    "# Set layer weights of the model\n",
    "for name in weights:\n",
    "  if model.get_layer(name) != None:\n",
    "    model.get_layer(name).set_weights(weights_dict[name])\n",
    "  elif model.get_layer(name) != None:\n",
    "    model.get_layer(name).set_weights(weights_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/nn4.small2.lrn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhinav\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "# model = load_model('./model/nn4.small2.lrn.h5')\n",
    "from keras.utils import CustomObjectScope\n",
    "import tensorflow as tf\n",
    "with CustomObjectScope({'tf': tf}):\n",
    "    model = load_model('./model/nn4.small2.lrn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>image_to_embedding</font> function        \n",
    "When the model is loaded with pre trained weights, then we can create the **128 dimensional embedding vectors** for all the face images stored in the \"imagesv1\" folder. **\"image_to_embedding\"** function pass an image to the Inception network to generate the embedding vector.\n",
    "\n",
    "By using a 128-neuron fully connected layer as its last layer, the model ensures that the output is an encoding vector of size 128. You then use the encodings the compare two face images as follows:\n",
    "\n",
    "<img src=\"siamese.jpg\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> <br> </u> <font color='purple'> By computing a distance between two encodings and thresholding, you can determine if the two pictures represent the same person</center></caption>\n",
    "\n",
    "So, an encoding is a good one if: \n",
    "- The encodings of two images of the same person are quite similar to each other \n",
    "- The encodings of two images of different persons are very different\n",
    "\n",
    "The triplet loss function formalizes this, and tries to \"push\" the encodings of two images of the same person (Anchor and Positive) closer together, while \"pulling\" the encodings of two images of different persons (Anchor, Negative) further apart. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_embedding(image, model):\n",
    "    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
    "    image = cv2.resize(image, (96, 96)) \n",
    "    img = image[...,::-1]   #rgb to bgr\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>recognize_face</font> function\n",
    "This function calculate similarity between the captured image and the images that are already been stored. It passes the image to the trained neural network to generate its embedding vector. Which is then compared with all the embedding vectors of the images stored by calculating L2 Euclidean distance. \n",
    "\n",
    "If the minimum L2 distance between two embeddings is less than a threshpld (here I have taken the threashhold as .68 (which can be adjusted) then we have a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(face_image, input_embeddings, model):   #  # change aso using cosine distance\n",
    "\n",
    "    embedding = image_to_embedding(face_image, model)\n",
    "    \n",
    "    minimum_distance = 200\n",
    "    minimum_distance1 = 0\n",
    "    name = None\n",
    "    \n",
    "    # Loop over  names and encodings.\n",
    "    for (input_name, input_embedding) in input_embeddings.items():\n",
    "        \n",
    "       #Euclidean Calculation\n",
    "        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n",
    "        \n",
    "        #Cosine Calculation\n",
    "        a = np.matmul(embedding,input_embedding.T)\n",
    "        b = np.sqrt(np.matmul(embedding,embedding.T))\n",
    "        c = np.sqrt(np.matmul(input_embedding,input_embedding.T))\n",
    "        cosine_dist = a / ( b * c )\n",
    "        \n",
    "        #print(dist1)\n",
    "        print (cosine_dist)\n",
    "        \n",
    "      \n",
    "        print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
    "        print('\\n')\n",
    "        \n",
    "        if euclidean_distance < minimum_distance and cosine_dist > minimum_distance1:\n",
    "            minimum_distance = euclidean_distance\n",
    "            minimum_distance1 = cosine_dist\n",
    "            name = input_name[0]\n",
    "            \n",
    "    \n",
    "    if minimum_distance < 0.50 and minimum_distance1 > 0.85:\n",
    "        return str(name)\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>create_input_image_embeddings</font> function\n",
    "This function generates 128 dimensional image ebeddings of all the images stored in the \"imagesv1\" directory by feed forwarding the images to a trained neural network. It creates a dictionary with key as the name of the face and value as embedding\n",
    "\n",
    "\n",
    "## About <font color=blue>recognize_faces_in_cam</font> function\n",
    "This function capture image from the webcam, detect a face in it and crop the image to have a face only, which is then passed to recognize_face function. \n",
    "<font color=blue>\n",
    "1. Compute the target encoding of the image from image_path\n",
    "2. Find the encoding from the database that has less distance then threshold i.e. the person are similar .\n",
    "    \n",
    "Here're some examples of distances between the encodings between three individuals:\n",
    "\n",
    "<img src=\"slide.jpg\" style=\"width:380px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def create_input_image_embeddings():       #  # Change folder --> folder \n",
    "    input_embeddings = {}\n",
    "\n",
    "    for file in glob.glob(\"imagesv1/*\"):\n",
    "        for file1 in glob.glob(file+\"/*\"):\n",
    "            person_name_image = os.path.splitext(os.path.basename(file1))[0]\n",
    "            person_name = os.path.splitext(os.path.basename(file))[0]\n",
    "#             print(person_name)\n",
    "            image_file = cv2.imread(file1, 1)\n",
    "            input_embeddings[person_name,person_name_image] = image_to_embedding(image_file, model)\n",
    "\n",
    "    return input_embeddings\n",
    "\n",
    "def recognize_faces_in_cam(input_embeddings):\n",
    "    \n",
    "\n",
    "    cv2.namedWindow(\"Face Recognizer\")\n",
    "\n",
    "   \n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    url =\"http://192.168.0.106:8080/shot.jpg\"\n",
    "    while True:\n",
    "        img_resp=requests.get(url)\n",
    "        img_arr=np.array(bytearray(img_resp.content),dtype=np.uint8)\n",
    "        img=cv2.imdecode(img_arr,-1)\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        \n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Loop through all the faces detected \n",
    "        identities = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x+w\n",
    "            y2 = y+h\n",
    "\n",
    "           \n",
    "            \n",
    "            face_image = img[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]\n",
    "            \n",
    "            identity = recognize_face(face_image, input_embeddings, model)\n",
    "            \n",
    "            \n",
    "\n",
    "            if identity is not None:\n",
    "                img = cv2.rectangle(img,(x1, y1),(x2, y2),(0,255,0),2)\n",
    "                cv2.putText(img, str(identity), (x1+5,y1-5), font, 1, (255,0,0), 2)\n",
    "        \n",
    "        key = cv2.waitKey(10)\n",
    "        cv2.imshow(\"Face Recognizer\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing the face image\n",
    "Following code captures 10 face images of the person. They all are stored in **\"imagesv1\"** folder with the name newuser with date containing all images. Select a good captured image from the set of 10 images. Rename it with the name of person and delete rest of them. This image will be used for recognizing the the identity of the person using one shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "#  #change - to get current time and date\n",
    "from datetime import datetime\n",
    "date_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "#  #change create a new folder to save images\n",
    "newpath = r'C:\\Users\\Abhinav\\Downloads\\Face-recognition-using-deep-learning-master\\imagesv1\\newUser'+date_time \n",
    "image_path = r'imagesv1\\newUser'+date_time\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "while(True):\n",
    "    ret, img = cam.read()\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        x1 = x\n",
    "        y1 = y\n",
    "        x2 = x+w\n",
    "        y2 = y+h\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (255,255,255), 2)     \n",
    "        count += 1\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(image_path+\"/User_\" + str(count) + \".jpg\", img[y1:y2,x1:x2])\n",
    "        cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(200) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 10: # Take 30 face sample and stop video\n",
    "         break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_embeddings = create_input_image_embeddings()\n",
    "recognize_faces_in_cam(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n",
    "- The pretrained model that I have used is by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Key Points :**:\n",
    "\n",
    "- Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem. \n",
    "- The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.\n",
    "- The same encoding can be used for verification and recognition. Measuring distances between two images' encodings allows you to determine whether they are pictures of the same person. \n",
    "\n",
    "-> If euclidian distance is greater it's of different person and if smaller means similar person.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
