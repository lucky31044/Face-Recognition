{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can download the pretrained weights from the following link \n",
    "#https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "#or you can find the detailed documentation https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "\n",
    "from keras.models import model_from_json\n",
    "model.load_weights('C:/Users/Abhay/Desktop/Face-recognition-using-deep-learning-master/vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def img_estim(img, thrshld):\n",
    "    is_light = np.mean(img) > thrshld\n",
    "    print(np.mean(img))\n",
    "    return 'light' if is_light else 'dark'\n",
    "\n",
    "def adjust_gamma(image, gamma=1):\n",
    "\n",
    "   invGamma = 1.0 / gamma\n",
    "   table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "   return cv2.LUT(image, table)\n",
    "\n",
    "\n",
    "def Gamma_correction(image) :\n",
    "#     f = cv2.imread(image,1)\n",
    "    f = image\n",
    "#     original = cv2.imread(image, 1)\n",
    "    original = image\n",
    "    if img_estim(f, 90) == 'dark' :\n",
    "        print('dark--------')\n",
    "        gamma = 2.0\n",
    "        adjusted = adjust_gamma(original, gamma=gamma)\n",
    "        return adjusted\n",
    "    else :\n",
    "        return original\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_embedding(image, model):\n",
    "#     image  =  Gamma_correction(image)\n",
    "#     image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
    "    image = cv2.resize(image, (224, 224)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(face_image, input_embeddings, model):   #  # change also using cosine distance\n",
    "   \n",
    "    \n",
    "    corrected_face_image = Gamma_correction(face_image)\n",
    "    embedding = image_to_embedding(corrected_face_image, model)\n",
    "    \n",
    "#     i =0\n",
    "#     if i == 0:\n",
    "#         cv2.imshow(\"Original Image\",face_image)\n",
    "#         cv2.imshow(\"Corrected Image\",corrected_face_image)\n",
    "#         e1 = image_to_embedding(corrected_face_image, model)\n",
    "#         e2 = image_to_embedding(face_image, model)\n",
    "#         x =np.linalg.norm(e1-e2) * 1000\n",
    "#         print('************')\n",
    "#         print(x)\n",
    "#         print('************')\n",
    "#         cv2.imshow(\"gammam image 1\", corrected_face_image)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "#         i =1\n",
    "    \n",
    "    \n",
    "    \n",
    "    minimum_distance = 200\n",
    "    minimum_distance1 = 0\n",
    "    name = None\n",
    "    \n",
    "    # Loop over  names and encodings.\n",
    "    for (input_name, input_embedding) in input_embeddings.items():\n",
    "        \n",
    "       \n",
    "        euclidean_distance = np.linalg.norm(embedding-input_embedding) * 1000\n",
    "        \n",
    "        a = np.matmul(embedding,input_embedding.T)\n",
    "        b = np.sqrt(np.matmul(embedding,embedding.T))\n",
    "        c = np.sqrt(np.matmul(input_embedding,input_embedding.T))\n",
    "        cosine_dist = a / ( b * c ) * 10000\n",
    "        \n",
    "        #print(dist1)\n",
    "        print (cosine_dist)\n",
    "        \n",
    "      \n",
    "        print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
    "\n",
    "        \n",
    "        if euclidean_distance < minimum_distance and cosine_dist > minimum_distance1:\n",
    "            minimum_distance = euclidean_distance\n",
    "            minimum_distance1 = cosine_dist\n",
    "            name = input_name[0]\n",
    "            \n",
    "    \n",
    "    if minimum_distance < 0.17 and minimum_distance1 > 9999.6:\n",
    "        return str(name)\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def create_input_image_embeddings():       #  # Change folder --> folder \n",
    "    input_embeddings = {}\n",
    "\n",
    "    for file in glob.glob(\"imagesv1/*\"):\n",
    "        for file1 in glob.glob(file+\"/*\"):\n",
    "            person_name_image = os.path.splitext(os.path.basename(file1))[0]\n",
    "            person_name = os.path.splitext(os.path.basename(file))[0]\n",
    "#             print(person_name)\n",
    "            image_file = cv2.imread(file1, 1)\n",
    "            input_embeddings[person_name,person_name_image] = image_to_embedding(image_file, model)\n",
    "\n",
    "    return input_embeddings\n",
    "\n",
    "def recognize_faces_in_cam(input_embeddings):\n",
    "    \n",
    "\n",
    "    cv2.namedWindow(\"Face Recognizer\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "   \n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    \n",
    "    while vc.isOpened():\n",
    "        _, frame = vc.read()\n",
    "        img = frame\n",
    "        height, width, channels = frame.shape\n",
    "\n",
    "        \n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Loop through all the faces detected \n",
    "        identities = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x+w\n",
    "            y2 = y+h\n",
    "\n",
    "           \n",
    "            \n",
    "            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n",
    "            identity = recognize_face(face_image, input_embeddings, model)\n",
    "            \n",
    "            \n",
    "\n",
    "            if identity is not None:\n",
    "                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,255,255),2)\n",
    "                cv2.putText(img, str(identity), (x1+5,y1-5), font, 1, (255,255,255), 2)\n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"Face Recognizer\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "#  #change - to get current time and date\n",
    "from datetime import datetime\n",
    "date_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "#  #change create a new folder to save images\n",
    "newpath = r'C:/Users/Abhay/Desktop/Face-recognition-using-deep-learning-master/imagesv1/newUser'+date_time \n",
    "image_path = r'imagesv1\\newUser'+date_time\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "  \n",
    "    \n",
    "count = 0\n",
    "while(True):\n",
    "    ret, img = cam.read()\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        x1 = x\n",
    "        y1 = y\n",
    "        x2 = x+w\n",
    "        y2 = y+h\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (255,255,255), 2)     \n",
    "        count += 1\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(image_path+\"/User_\" + str(count) + \".jpg\", img[y1:y2,x1:x2])\n",
    "        cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(200) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 10: # Take 30 face sample and stop video\n",
    "         break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_embeddings = create_input_image_embeddings()\n",
    "recognize_faces_in_cam(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
